Phase 2 Technical Implementation Report: Risk Calculation Engine Architecture and Engineering Specification1. Introduction: The Imperative for Quantitative Risk ComputationThe evolution of physical security from a subjective art to a data-driven science necessitates the development of robust computational backends capable of quantifying exposure. Phase 2 of the Replit-based security assessment platform, "Build Risk Calculation Engine," represents the pivotal transition from passive data collection to active risk intelligence. While Phase 1 established the mechanism for gathering facility data via surveys, that data remains static and actionable only through manual interpretation. The objective of Phase 2 is to operationalize this data, transforming raw survey responses into calculated risk metrics—Inherent, Current, and Residual—that align with industry standards such as the Harmonized Threat and Risk Assessment (HTRA) methodology and ASIS International guidelines.1This report details the exhaustive technical architecture required to build this engine. It analyzes the mathematical models underpinning risk quantification, specifically the interaction between threat likelihood, asset impact, and the weighted effectiveness of control systems. It subsequently translates these theoretical models into concrete software engineering tasks, specifying the database schema design in PostgreSQL/Prisma, the API architecture in Next.js, and the algorithmic logic required to wire survey findings directly into risk mitigation workflows. The successful execution of this phase will provide stakeholders with a dynamic, defensible, and audit-ready view of their security posture, moving beyond binary compliance checklists to nuanced, quantitative risk analysis.41.1 Methodological Alignment and Compliance StandardsThe architecture of the Risk Calculation Engine is not arbitrary; it is rigorously aligned with established risk management frameworks to ensure the legal and operational defensibility of its outputs. The primary frameworks influencing this design are the NIST SP 800-30 Guide for Conducting Risk Assessments and the Government of Canada’s Harmonized Threat and Risk Assessment (HTRA) methodology.1 These standards dictate that risk is a function of threat, vulnerability, and impact, modified by the effectiveness of safeguards.The application of these standards within a software context requires a shift from the qualitative "High/Medium/Low" labels often used in executive summaries to precise numerical scoring engines in the backend. For instance, while a user might see a "High" risk label, the database must store this as a floating-point value (e.g., 18.5 on a 25-point scale) to allow for the granular measurement of risk reduction achieved by proposed treatments.6 Furthermore, the system must distinguish between types of controls—Preventive, Detective, and Corrective—assigning different mathematical weights to each based on their ability to interdict threats versus merely recording them.8 This distinction is critical for the "No BS" assessment philosophy, which prioritizes controls that actually stop attacks over those that essentially serve as "security theater".10 By embedding these weighted calculations directly into the schema and logic, the engine enforces a rigorous security philosophy through code.2. Mathematical Modeling of Physical Security RiskThe core requirement of the Phase 2 engine is to implement a mathematical model that serves as the "source of truth" for risk scoring. This model must be deterministic, meaning that the same set of inputs (Asset Value, Threat Likelihood, and Control Configuration) must always yield the same Risk Score. This determinism enables historical trending and comparative analysis across different facilities.2.1 The Three-Stage Risk Calculation LifecycleThe engine will compute risk in three distinct stages, providing a narrative arc for the user: "How bad could it be?" (Inherent), "How safe are we now?" (Current), and "How safe could we be?" (Residual).2.1.1 Inherent Risk ($R_{inh}$)Inherent risk represents the raw exposure of an asset before any safeguards are considered. It serves as the baseline for determining the Return on Investment (ROI) of security controls. If the inherent risk is low, expensive controls are unjustified.The formula implemented in the logic layer will be:$$R_{inh} = L \times I$$Where:$L$ (Likelihood) is a quantified estimate of the probability of a threat event initiating.$I$ (Impact) is a quantified estimate of the magnitude of harm should the event occur.The system will utilize a 5x5 matrix approach, where both variables are scored on an integer scale of 1 to 5. This yields a maximum Inherent Risk score of 25. The definitions for these scales are derived from standard safety and security matrices.6ScoreLikelihood Definition (L)Impact Definition (I)1Rare (Once in >10 years)Negligible (No injury, <$1k loss)2Unlikely (Once in 5-10 years)Minor (First aid, <$10k loss)3Possible (Once in 1-5 years)Moderate (Medical treatment, <$100k loss)4Likely (Once a year)Major (Disability, <$1M loss)5Almost Certain (>Once a year)Catastrophic (Fatality, >$1M loss)2.1.2 Current Risk ($R_{cur}$) and Control Effectiveness ($C_e$)Current Risk is the most complex calculation as it introduces the variable of Control Effectiveness ($C_e$). In many simplistic systems, controls are treated as binary switches (On/Off). However, research into control efficacy demonstrates that different types of controls reduce risk to varying degrees. A locked door (Preventive) is physically more effective at stopping an intrusion than a camera (Detective), which only records it.The Calculation Engine will utilize a Weighted Reduction Model. The effectiveness of a specific control is determined by its type and its implementation quality (derived from survey answers).$$C_e = \sum_{n=1}^{k} (W_{type} \times S_{imp})$$Where:$W_{type}$ is the coefficient of effectiveness for the control category. Based on HTRA and physical security engineering principles, Preventive controls are weighted highest, followed by Detective, then Procedural.2$S_{imp}$ is the Implementation Score derived from the survey (0.0 for "No", 0.5 for "Partial", 1.0 for "Yes").14The Current Risk is then calculated by applying the aggregate effectiveness as a reduction factor against the Inherent Risk 5:$$R_{cur} = R_{inh} \times (1 - \min(C_e, 0.95))$$Note: The engine caps aggregate effectiveness at 0.95 (95%) to reflect the reality that no physical security system is impenetrable; a non-zero residual risk always remains.52.1.3 Residual Risk ($R_{res}$)Residual Risk allows the system to model the future. It recalculates the risk formula including proposed controls (Treatments) that have not yet been implemented.$$R_{res} = R_{cur} \times (1 - T_e)$$Where $T_e$ is the projected effectiveness of the treatment plan. This value drives the business case for security investment by showing the delta between Current and Residual risk.42.2 Weighted Control Effectiveness TaxonomyTo implement the $W_{type}$ variable, the database must support a taxonomy of controls. The engine will enforce specific weightings that reflect the "No BS" philosophy of prioritizing engineered controls over administrative ones.Control TypeWeight (Wtype​)Rationale and SourcePreventive0.40Physical barriers (fences, locks) that physically impede the threat. Highest weight due to direct interdiction capability.2Detective0.20Systems (CCTV, Alarms) that identify an event. Lower weight because they require a human response to stop the threat.8Procedural0.10Policies and training. Lowest weight due to high susceptibility to human error and social engineering.2Corrective0.15Mechanisms to restore systems (Backups, Fire Suppression). focused on impact reduction rather than likelihood reduction.9This weighted logic ensures that a facility with only written policies (Procedural) will calculate a much higher Current Risk than a facility with physical barriers (Preventive), even if both have the same number of "controls" in place.23. Technical Architecture and Stack StrategyThe implementation of Phase 2 will take place within the existing Replit environment, utilizing a specific stack optimized for rapid development and type safety. The architectural decisions are driven by the need for tight integration between the frontend survey interface and the backend calculation logic.3.1 The Replit StackRuntime: Node.js is the execution environment, chosen for its non-blocking I/O which is suitable for handling multiple concurrent API requests during a risk assessment session.18Framework: Express.js (or Next.js API routes depending on the specific project configuration within Replit) provides the RESTful endpoint structure. The use of a standard HTTP router allows for the easy application of middleware for authentication and input validation.19Database: PostgreSQL (via Neon) provides the relational integrity required for the complex many-to-many relationships between controls and risk scenarios. The ACID compliance of Postgres is essential for ensuring that risk scores are reliably stored and transactions (like updating a scenario) are atomic.20ORM: Prisma is the Object-Relational Mapper of choice. Its schema-first approach allows for the generation of strict TypeScript definitions, ensuring that the frontend and backend share the exact same data structures for Risk Entities. This prevents a class of errors where the API returns a field (e.g., inherentRisk) that the frontend does not expect.213.2 State Management and Data FlowThe application utilizes a "Server-Authoritative" state model. While the frontend (React) will display the risk scores, it will not calculate them. All calculations occur on the server within the Storage Layer. This design pattern prevents client-side manipulation of risk scores and ensures that the logic described in Section 2 is applied consistently regardless of the client device.18Data flow follows the Request-Response cycle:Trigger: User updates a survey answer or modifies a scenario parameter.Request: Frontend sends a POST or PATCH to the API.Processing: The API invokes the Calculation Engine.Persistence: The Engine computes the new scores and updates the PostgreSQL database via Prisma.Response: The API returns the updated Risk Entity.Update: The frontend (using TanStack Query) invalidates its cache and refetches the new data.184. Task 1: Detailed Schema Design and ImplementationThe first engineering task is the expansion of the shared/schema.ts file. This file acts as the contract for the data layer. The schema must support the polymorphic nature of risks (where one asset faces multiple threats) and the aggregation of controls.4.1 Relational StrategyThe schema utilizes a hierarchical model rooted in the Assessment entity.Assessment (1) -> (N) RiskAssets: An assessment contains an inventory of assets.RiskAsset (1) -> (N) RiskScenarios: Each asset is evaluated against multiple threat scenarios.RiskScenario (1) -> (N) Treatments: Each scenario can have multiple mitigation plans.ControlLibrary (N) <-> (N) RiskScenarios: (Implicit Link) Controls mitigate scenarios, linked logically via the threat_type tag.4.2 Table SpecificationsThe following tables must be defined in the Prisma schema. The use of the Float type for scoring fields is mandatory to support the decimal precision of the weighted calculation model.214.2.1 RiskAssets TableThis table stores the "Value" component of the risk formula.FieldTypeDescriptionConstraintsidIntPrimary Key@id @default(autoincrement())assessmentIdIntForeign Key@relation(fields: [assessmentId], references: [id])nameStringAsset NameNot NullcategoryStringType (People, IT, Physical)Not NullcriticalityFloatImpact Value ($I$)Default 1.0, Min 1, Max 5descriptionStringContextual detailsNullableownerStringResponsible partyNullableRationale: The criticality field corresponds directly to the $I$ variable in the $R=L \times I$ formula. Separating the asset from the scenario allows the user to define an asset once (e.g., "Main Server") and apply it to multiple threats (Theft, Fire, Flood) without re-entering the asset's value.234.2.2 RiskScenarios TableThis is the central junction table where the calculations are stored.FieldTypeDescriptionConstraintsidIntPrimary Key@id @default(autoincrement())assessmentIdIntForeign KeyIndexed for performanceriskAssetIdIntForeign Key@relation(fields: [riskAssetId], references: [id])threatTypeStringEnum or Stringe.g., "Unauthorized Access", "Vandalism"likelihoodFloatProbability ($L$)Not Null, Min 1, Max 5impactFloatConsequence ($I$)Inherited from Asset but overrideableinherentRiskFloatComputed ($L \times I$)Stored for reporting speedcurrentRiskFloatComputed ($R_{inh} \times (1-C_e)$)Stored valueresidualRiskFloatComputed ($R_{cur} \times (1-T_e)$)Stored valuenotesStringAssessor justificationNullablecreatedAtDateTimeAudit timestamp@default(now())Rationale: Storing the computed risks (inherentRisk, currentRisk, residualRisk) creates a slight data redundancy but significantly improves read performance for the dashboard. It avoids the need to re-calculate complex weighted sums every time the user views the summary page. This "materialized view" approach is common in analytics-heavy applications.244.2.3 Treatments TableThis table manages the future state of the security posture.FieldTypeDescriptionConstraintsidIntPrimary Key@id @default(autoincrement())riskScenarioIdIntForeign Key@relationtitleStringMitigation NameNot NulldescriptionStringImplementation detailsNullablecostFloatImplementation costNullableeffectivenessFloatProjected reduction ($T_e$)0.0 - 1.0 scalestatusStringEnum"Proposed", "Approved", "Implemented"4.2.4 Vulnerabilities (Optional/Expansion)While not strictly required for the basic calculation, a Vulnerabilities table can link specific survey findings (e.g., "Door 4 lock broken") to the scenario. This supports the more granular $Risk = Threat \times Vulnerability \times Impact$ formula variants found in advanced snippets.5 For this phase, vulnerabilities will primarily be inferred from negative survey responses.4.3 Migration StrategyFollowing the definition of these schemas, the implementation requires running a database migration. In the Replit/Neon environment, this is typically handled by the command npm run db:push or npx prisma migrate dev. This action translates the TypeScript schema definitions into SQL CREATE TABLE commands and executes them against the PostgreSQL instance.20 It is critical to verify that the migration script includes foreign key constraints (ON DELETE CASCADE) to ensure that deleting an Assessment automatically cleans up all associated risk data, preventing orphaned records.215. Tasks 2 & 4: Backend Logic and Calculation EngineThe heart of Phase 2 is the implementation of the calculation logic within the storage layer (server/storage.ts) and the calculation helper (shared/riskCalculations.ts). This section combines Task 2 (Storage CRUD) and Task 4 (Wiring the Engine) because they are functionally inseparable; the CRUD operations must trigger the calculations.5.1 The Calculation Helper Module (shared/riskCalculations.ts)This module will export pure functions that encapsulate the risk math. By keeping these functions pure (no database side effects), they can be unit-tested easily.TypeScript// shared/riskCalculations.ts implementation logic

/**
 * Standardizes the calculation of Inherent Risk.
 * Ensures the result stays within the 1-25 matrix bounds.
 */
export function calculateInherentRisk(likelihood: number, impact: number): number {
  const rawScore = likelihood * impact;
  return Math.min(Math.max(rawScore, 1), 25);
}

/**
 * Applies the Weighted Control Effectiveness reduction.
 * 
 * @param inherentRisk - The baseline risk score (1-25)
 * @param controlEffectiveness - Aggregate score (0.0 - 1.0) calculated from survey
 * @returns Current Risk score
 */
export function calculateCurrentRisk(inherentRisk: number, controlEffectiveness: number): number {
  // Cap effectiveness at 0.95 per industry best practice (residuum principle)
  const effectiveReduction = Math.min(controlEffectiveness, 0.95);
  return inherentRisk * (1 - effectiveReduction);
}

/**
 * Aggregates individual control scores into a total effectiveness rating.
 * 
 * @param controls - Array of control objects with 'weight' and 'isImplemented' status
 */
export function aggregateControlEffectiveness(controls: Array<{weight: number, status: number}>): number {
  return controls.reduce((acc, control) => {
    return acc + (control.weight * control.status);
  }, 0);
}
Insight: The aggregateControlEffectiveness function is where the "No BS" weighting logic is applied. The weight parameter will be fed values like 0.4 (Preventive) or 0.1 (Procedural) based on the metadata from the Control Library.25.2 Storage Layer Implementation (server/storage.ts)The storage layer acts as the interface between the API and the database. It implements the IStorage interface. The critical innovation here is the "Cascading Update" pattern.5.2.1 The Cascading Update LogicWhen a RiskScenario is created or updated, the storage method cannot simply write the values to the database. It must perform a calculation chain:Fetch Context: Retrieve the RiskAsset to get the default impact if not provided.Calculate Inherent: Compute $L \times I$.Fetch Controls: Query the SurveyResponses linked to this Assessment. Filter for questions linked to controls that match the scenario's threatType (e.g., retrieve all answers related to "Access Control" if the threat is "Intrusion").Compute Effectiveness: Map the survey answers (Yes=1, No=0) to the control weights (Preventive=0.4, etc.) and sum them.Calculate Current: Apply the reduction formula.Save: Write the fully computed record to the RiskScenarios table.This logic ensures that the database always contains the net risk state. If a user later changes a survey answer (e.g., changes "Do you have CCTV?" from "No" to "Yes"), a trigger or a service hook must re-run this calculation for all linked scenarios.5.3 Control-Survey Mapping LogicA key challenge identified in the research is linking the generic survey questions to specific risk scenarios. The implementation addresses this by adding metadata to the ControlLibrary.Control Tagging: Each control in the library will have a threat_mitigation_tags array (e.g., ``).Query Logic: When calculating risk for an "Intrusion" scenario, the backend queries: SELECT * FROM SurveyResponses WHERE control.tags CONTAINS 'Intrusion'.This allows for a dynamic relationship where one control can mitigate multiple risks, reflecting the interconnected nature of physical security systems.266. Task 3: API Route Architecture and SecurityThe API layer exposes the calculated risk data to the frontend. Built on Express (within the Replit/Next.js wrapper), these routes must be secure, validated, and RESTful.6.1 Endpoint StrategyThe API will be organized hierarchically under the /api/assessments/:id/ path, reinforcing that all risk data belongs to a specific assessment context.GET /api/assessments/:id/risk-summaryPurpose: Aggregates data for the dashboard.Logic: Returns the average Inherent, Current, and Residual risk across all scenarios. It creates the "Top 5 Risks" list by sorting currentRisk descending.28Optimization: This endpoint should use a single efficient SQL query (using Prisma's groupBy or raw query) rather than fetching all rows and filtering in JavaScript, ensuring performance as the dataset grows.21POST /api/assessments/:id/risk-scenariosPurpose: Creates a new risk scenario.Validation: Uses Zod to ensure likelihood and impact are integers between 1 and 5. Rejects inputs that violate the matrix logic.29Trigger: Invokes the createRiskScenario storage method, which triggers the calculation chain described in 5.2.1.POST /api/assessments/:id/treatmentsPurpose: Adds a mitigation plan.Logic: Accepts a riskScenarioId and effectiveness estimate. Updates the parent scenario's residualRisk immediately.6.2 Security and Access ControlGiven the sensitivity of security risk data, rigorous access control is mandatory.Middleware: An ensureAssessmentOwnership middleware function will run before every risk route. It checks the session userId against the assessment.ownerId in the database.Prevention: This prevents IDOR attacks where a malicious user creates a risk scenario on an assessment they do not own by manipulating the API URL.29Headers: The API will enforce standard security headers (e.g., Content-Security-Policy) and disable caching for these sensitive endpoints to prevent data leakage on shared computers.197. Frontend Integration: Visualization and Workflow (Tasks 5, 6, 7)The frontend implementation focuses on usability and visualization. It translates the dry numbers from the backend into intuitive graphics that communicate risk urgency to non-technical stakeholders.7.1 The EnhancedRiskAssessment ComponentThe existing placeholder component will be replaced with a fully reactive data dashboard.Data Fetching: Utilizing TanStack Query (React Query), the component will fetch the risk data. The staleTime should be set to 0 for risk data to ensure that any change in the survey (performed in a different tab) is immediately reflected in the risk calculations upon window focus.18Visual Matrix: A 5x5 Heatmap Grid will be rendered using CSS Grid. Cells will be colored from Green (1-4) to Red (20-25) based on the standard risk matrix colors.12 Assets will be plotted as dots on this grid, allowing users to visually cluster high-risk items.7.2 Survey-to-Risk Workflow Gating (Task 6)To prevent "Garbage In, Garbage Out," the system must enforce a workflow order.The "Lock" Mechanism: The Risk Assessment tab in the navigation bar will be disabled (greyed out) until the backend flag facilitySurveyCompleted is true.The "Unlock" Event: When the survey reaches 100% completion, the backend updates this flag. The frontend receives this update and enables the tab, signaling to the user that "Data collection is complete; Analysis can begin."Auto-Population: Upon first access, the frontend will trigger a "Hydration" process. It will inspect the facility type (e.g., "Office") and automatically generate a set of baseline risk scenarios (e.g., "Laptop Theft", "Social Engineering") via an API call to POST /api/assessments/:id/init-risks. This reduces the "Blank Page Syndrome" for users.47.3 Treatment Planning Interface (Task 7)This interface drives the "Residual Risk" calculation.Interactive Sliders: The UI will provide a slider for "Estimated Treatment Effectiveness." As the user drags the slider (e.g., increasing effectiveness from 50% to 80%), the "Residual Risk" bar chart will animate downwards in real-time.Psychological Impact: This immediate visual feedback demonstrates the value of the proposed security investment. It answers the executive question: "If I give you this budget, how much safer do we get?".4Cost-Benefit Analysis: Input fields for "Estimated Cost" allow the system to calculate a "Risk Reduction per Dollar" metric, sorting treatments by efficiency.88. Conclusion: From Data to DecisionsThe architecture detailed in this report represents a significant leap in the maturity of the security assessment platform. By implementing Phase 2, the system evolves from a digital clipboard into an analytical engine. The rigorous schema design ensures data integrity, the weighted calculation logic enforces professional security standards (HTRA/ASIS), and the reactive frontend empowers users to make evidence-based decisions.This implementation aligns perfectly with the "No BS" philosophy by stripping away subjective ambiguity. Controls are weighted by what they do (Prevent vs. Detect), not what they promise. Risks are scored by math, not feeling. The result is a tool that provides a defensible, transparent, and actionable roadmap for improving physical security posture, grounded in verified survey data and calculated with engineering precision. The completion of these seven tasks will establish the backend foundation necessary for all future analytics, reporting, and machine learning capabilities of the platform.